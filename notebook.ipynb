{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KxYAnVM1Ag5T"
      },
      "source": [
        "# Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Hz6eZixOS5",
        "outputId": "06e5d9da-0865-4e4f-ca11-0d349b27745a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fugashi[unidic-lite] in /usr/local/lib/python3.10/dist-packages (1.2.1)\n",
            "Collecting unidic-lite (from fugashi[unidic-lite])\n",
            "  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: unidic-lite\n",
            "  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658818 sha256=1461cb4789b771ded4696c32d23e7bd6fa766db0fe8c1946604f78ee580c645e\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/e8/68/f9ac36b8cc6c8b3c96888cd57434abed96595d444f42243853\n",
            "Successfully built unidic-lite\n",
            "Installing collected packages: unidic-lite\n",
            "Successfully installed unidic-lite-1.0.8\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: anki in /usr/local/lib/python3.10/dist-packages (2.1.65)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from anki) (4.11.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from anki) (4.4.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from anki) (3.4.3)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from anki) (3.9.1)\n",
            "Requirement already satisfied: protobuf>=4.21 in /usr/local/lib/python3.10/dist-packages (from anki) (4.23.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from anki) (2.27.1)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.10/dist-packages (from anki) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->anki) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->anki) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->anki) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->anki) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->anki) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->anki) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install fugashi[unidic-lite]\n",
        "%pip install anki"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gRUqOV_xKRl"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from fugashi import Tagger\n",
        "from anki import collection\n",
        "import zipfile\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "TigyrRXgxKRm"
      },
      "outputs": [],
      "source": [
        "def import_frequencies(filename,word_col,freq_col,separator,start_line,ascending):\n",
        "  unicode_ranges = [\n",
        "    r'\\u3005-\\u3006',   # Kanji punctuation\n",
        "    r'\\u3040-\\u309F',   # Hiragana\n",
        "    r'\\u30A0-\\u30FA',   # Katakana\n",
        "    r'\\u4E00-\\u9FAF',   # Kanji\n",
        "  ]\n",
        "  filter = '[^' + ''.join(unicode_ranges) + ']'\n",
        "\n",
        "  frequencies = {}\n",
        "  tagger = Tagger()\n",
        "\n",
        "  file = open(filename, 'r', encoding=\"utf8\")\n",
        "  for l, line in enumerate(file,1):\n",
        "    if(l>=start_line):\n",
        "      split = line.split(separator)\n",
        "      word = split[word_col][:-1]\n",
        "      frequency = float(split[freq_col])\n",
        "\n",
        "      match = re.search(filter,word) != None # filter out words with invalid characters\n",
        "      if(match == False):\n",
        "        lemma = str(tagger(word)[0].feature.lemma) # lemmatise word to standardise form\n",
        "        frequencies[lemma] = frequencies.get(lemma, 0) + frequency # add value to key rather than overriding (in the case of multiple lemma occurrences)\n",
        "\n",
        "  if(ascending): # reverse association of key to value if frequency list ascending\n",
        "    frequencies = {key: list(reversed(list(frequencies.values())))[i] for i, key in enumerate(frequencies)}\n",
        "  return dict(sorted(frequencies.items(), key=lambda item: item[1], reverse=True)) # return sorted by value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ZZOB6laExKRn"
      },
      "outputs": [],
      "source": [
        "def import_knowledge(filename,format,start_line=None,col=None,separator=None,deck=None,field=None):\n",
        "  knowledge = set()\n",
        "  tagger = Tagger()\n",
        "\n",
        "  if(format==\"anki\"):\n",
        "    col = collection.Collection(filename)\n",
        "    note_ids = col.find_notes(f\"deck:{deck}\") # gets note ids of deck\n",
        "    for note_id in note_ids:\n",
        "      note = col.get_note(note_id) # get note of note id\n",
        "      if field in note:\n",
        "        for word in tagger(note[field]): # append every unique lemma to knowledge\n",
        "          lemma = word.feature.lemma\n",
        "          if(lemma not in knowledge):\n",
        "            knowledge.add(lemma)\n",
        "  else:\n",
        "    file = open(filename, 'r', encoding=\"utf8\")\n",
        "    for l, line in enumerate(file,1):\n",
        "      if(l>=start_line-1):\n",
        "        if(format==\"tabular\"): # only focus on given column if tabular data\n",
        "          text = line.split(separator)[col]\n",
        "        elif(format==\"full\"):\n",
        "          text = line\n",
        "        for word in tagger(text): # append every unique lemma to knowledge\n",
        "          lemma = word.feature.lemma\n",
        "          if(lemma not in knowledge):\n",
        "            knowledge.add(lemma)\n",
        "\n",
        "  return knowledge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONQS1XpqxKRn"
      },
      "outputs": [],
      "source": [
        "def import_content(filename=None, content_string=None):\n",
        "  if(filename!=None): # opens file if passed in\n",
        "    file = open(filename, 'r', encoding=\"utf8\")\n",
        "    text = file.read()\n",
        "  if(content_string!=None): # reads string if passed in\n",
        "    text = content_string\n",
        "\n",
        "  content = []\n",
        "\n",
        "  lines = text.split(\"\\n\")\n",
        "  for line in lines:\n",
        "    sentences = line.split(\"。\")\n",
        "    for sentence in sentences:\n",
        "      content.append(sentence)\n",
        "\n",
        "  return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBjCsks6xKRn"
      },
      "outputs": [],
      "source": [
        "def import_dictionary(zipname):\n",
        "  term_col = 0\n",
        "  def_col = 5\n",
        "  reading_col = 1\n",
        "\n",
        "  dictionary = {}\n",
        "\n",
        "  with zipfile.ZipFile(zipname, 'r') as z:\n",
        "      for filename in z.namelist():\n",
        "          with z.open(filename) as f:\n",
        "              if filename.startswith('term'): # iterate through every term json file in dictionary zip\n",
        "                  data = f.read()\n",
        "                  json_data = json.loads(data)\n",
        "\n",
        "                  for entry in json_data:\n",
        "                    if(entry[def_col] not in dictionary.get(entry[term_col],[])): # prevent duplicate entries\n",
        "                      dictionary.setdefault(entry[term_col],[]).append({\"content\":entry[def_col],\"reading\":entry[reading_col]}) # add entry and reading to dictionary\n",
        "\n",
        "  return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIndvqKQxKRo"
      },
      "outputs": [],
      "source": [
        "def generate_recommendations(filter_katakana,pos_filter,frequencies,knowledge,content,dictionary):\n",
        "  recommendations = {}\n",
        "  tagger = Tagger()\n",
        "\n",
        "  for s, sentence in enumerate(content):\n",
        "    for word in tagger(sentence):\n",
        "      if(filter_katakana):\n",
        "        katakana = re.search(r\"[\\u30A0-\\u30FA]\",str(word)) != None # filter out words with katakana\n",
        "      else:\n",
        "        katakana = False\n",
        "\n",
        "      pos = word.pos.split(\",\")[0] # filter out words whose part-of-speech tags are blacklisted\n",
        "      blacklist = pos in pos_filter\n",
        "      lemma = str(word.feature.lemma)\n",
        "      if(lemma.find(\"-\")!=-1):\n",
        "          lemma = lemma[:lemma.find(\"-\")] # unidic adds english translation after \"-\" to certain katakana lemmas- remove this\n",
        "      definition = dictionary.get(lemma,[])\n",
        "\n",
        "      if(not blacklist and not katakana and lemma != \"None\" and definition!=[] and lemma not in knowledge):\n",
        "        if(lemma not in recommendations): # create recommendation entry for word\n",
        "          frequency = frequencies.get(lemma,0)\n",
        "          recommendations[lemma] = {\"freq\": frequency, \"sent\":{}, \"pos\":pos, \"def\":definition}\n",
        "\n",
        "        search_index = recommendations[lemma][\"sent\"].get(sentence,[(0,0)])[-1][1] # searches for index of word occurence in sentence from index of final occurrence currently found\n",
        "        word_index = sentence.find(str(word),search_index)\n",
        "        recommendations[lemma][\"sent\"].setdefault(sentence, []).append((word_index,word_index+len(str(word)))) # create list of word start and end occurrence indices for sentence if nonexistent, otherwise append\n",
        "\n",
        "  recommendations = dict(sorted(recommendations.items(), key=lambda item: item[1][\"freq\"], reverse=True)) # sort by frequency\n",
        "  if(recommendations!={}):\n",
        "    max_frequency = recommendations[next(iter(recommendations))][\"freq\"] # max frequency is first value\n",
        "    if(max_frequency!=0):\n",
        "      recommendations = {outer_k: {inner_k: (inner_v / max_frequency if inner_k==\"freq\" else inner_v) for inner_k, inner_v in outer_v.items()} for outer_k, outer_v in recommendations.items()} # normalise frequencies\n",
        "\n",
        "  return recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "rh6oPs_uxKRo"
      },
      "outputs": [],
      "source": [
        "def print_recommendations(recommendations, content, display):\n",
        "  for r, rec in enumerate(recommendations.items(),1):\n",
        "    print(str(r) + \" \\033[1m\" + rec[0]+\"\\033[0m\")\n",
        "\n",
        "    if(display[\"Frequency Ranking\"]):\n",
        "      print(\"\\033[1m\"+\"Frequency Ranking: \"+\"\\033[0m\"+str(format(rec[1][\"freq\"],\".3f\")))\n",
        "\n",
        "    if(display[\"Part of Speech\"]):\n",
        "      print(\"\\033[1m\"+\"Part of Speech: \"+\"\\033[0m\"+ \",\".join(rec[1][\"pos\"]))\n",
        "\n",
        "    if(display[\"Definition\"]):\n",
        "      print(\"\\033[1m\"+\"Definition:\"+\"\\033[0m\")\n",
        "      for d, definition in enumerate(rec[1][\"def\"],1):\n",
        "        if(definition[\"reading\"]!=\"\"):\n",
        "          reading = \"(\" + definition[\"reading\"] + \")  \"\n",
        "        else:\n",
        "          reading = \"\"\n",
        "        print(\" \" + str(d) + \" \" + reading + \", \".join(definition[\"content\"]))\n",
        "\n",
        "    if(display[\"Source Sentences\"]):\n",
        "      print(\"\\033[1m\"+\"Source Sentences:\"+\"\\033[0m\")\n",
        "\n",
        "      for s, sentence in enumerate(rec[1][\"sent\"],1):\n",
        "        sent_string = \" \" + str(s) + \" \"\n",
        "        string_index = 0\n",
        "        for word_indices in rec[1][\"sent\"][sentence]:\n",
        "          sent_string += sentence[string_index:word_indices[0]] + \"\\033[1m\\033[94m\" + sentence[word_indices[0]:word_indices[1]] + \"\\033[0m\"\n",
        "          string_index = word_indices[1]\n",
        "        sent_string += sentence[string_index:]\n",
        "\n",
        "        print(sent_string)\n",
        "\n",
        "    print(\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Tzjm9XAn6E"
      },
      "source": [
        "# Execution"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDXKGyq-Awgp"
      },
      "source": [
        "Execution Instructions:\n",
        "\n",
        "Import frequency, knowledge, content, and dictionary file, and provide filenames to respective functions.\n",
        "\n",
        "Sample knowledge and content files are provided.\n",
        "\n",
        "Run all cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNpTfyaWxKRo",
        "outputId": "a6005d6e-edb5-44de-f3c2-9385b92fb963"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 \u001b[1m好き\u001b[0m\n",
            "\u001b[1mFrequency Ranking: \u001b[0m1.000\n",
            "\u001b[1mDefinition:\u001b[0m\n",
            " 1 (すき)  liked, well-liked, favourite, favorite\n",
            " 2 (すき)  in love (with), loved, romantically interested (in)\n",
            " 3 (すき)  faddism, eccentricity\n",
            " 4 (すき)  the way one likes, (as) it suits one\n",
            " 5 (すき)  refined taste, elegant pursuits\n",
            " 6 (ずき)  love of, affection for, enthusiast for, lover of, fan, -phile\n",
            " 7 (ずき)  being attractive to, being liked by\n",
            "\u001b[1mSource Sentences:\u001b[0m\n",
            " 1 猫が\u001b[1m\u001b[94m好き\u001b[0mですが犬が\u001b[1m\u001b[94m好き\u001b[0mではない\n",
            "\n",
            "2 \u001b[1m猫\u001b[0m\n",
            "\u001b[1mFrequency Ranking: \u001b[0m0.311\n",
            "\u001b[1mDefinition:\u001b[0m\n",
            " 1 (ねこ)  cat (esp. the domestic cat, Felis catus)\n",
            " 2 (ねこ)  shamisen\n",
            " 3 (ねこ)  geisha\n",
            " 4 (ねこ)  wheelbarrow\n",
            " 5 (ねこ)  clay bed-warmer\n",
            " 6 (ねこ)  bottom, submissive partner of a homosexual relationship\n",
            " 7 (ねこま)  cat\n",
            "\u001b[1mSource Sentences:\u001b[0m\n",
            " 1 これは\u001b[1m\u001b[94m猫\u001b[0mです\n",
            " 2 \u001b[1m\u001b[94m猫\u001b[0mが好きですが犬が好きではない\n",
            "\n"
          ]
        }
      ],
      "source": [
        "frequencies = import_frequencies(filename=\"sample_data/frequencies.txt\", word_col=2, freq_col=1, separator=\" \", start_line=5, ascending=False)\n",
        "knowledge = import_knowledge(filename=\"sample_data/knowledge.txt\", col=0, separator=\",\", format=\"tabular\", start_line=1)\n",
        "content = import_content(filename=\"content.txt\")\n",
        "dictionary = import_dictionary(\"jmdict_english.zip\")\n",
        "\n",
        "pos_filter = [\"感動詞\",\"補助記号\",\"助詞\",\"記号\"]\n",
        "filter_katakana = False\n",
        "\n",
        "recommendations = generate_recommendations(filter_katakana, pos_filter, frequencies, knowledge, content, dictionary)\n",
        "\n",
        "display = {\n",
        "    \"Frequency Ranking\":True,\n",
        "    \"Part of Speech\":False,\n",
        "    \"Source Sentences\":True,\n",
        "    \"Definition\":True,\n",
        "}\n",
        "\n",
        "print_recommendations(recommendations, content, display)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
